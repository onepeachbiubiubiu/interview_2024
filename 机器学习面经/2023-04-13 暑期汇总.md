# 暑期实习内容汇总

# 1. 总结

这一个半月投了有将近100来家大小厂的机器学习的职位，大部分都是处于没有动静的状态，给笔试的大部分也是海笔，能进面试的也就没几家了。大部分小厂进了一面，大厂只有美团、百度、腾讯和蚂蚁。

1. 百度由于自己作死没有写出来面试中的算法题直接一面挂了；
2. 腾讯的WXG部门则是感觉他们自己作死，面试官是个新手，面试没有自我介绍和反问环节，一开始让30分钟写四道算法题，一道简单题三道中等题，写完后开始问C++的内容，没有涉及简历的内容，面完直接就挂掉了。这是唯一一家面试体验特别差的。
3. 美团已经拿到offer啦，两场的面试官人都非常好，面试体验极佳，很给人信心，一面有两道中等算法题，二面一道困难题，问的都是简历上的内容和一些扩展。
4. 蚂蚁也已经二面了，一面面试官很好，二面就不太好，二面面试官一直在说简历项目和岗位不匹配，但是我是等了三个星期才接到二面的，不匹配为什么不直接拒掉。蚂蚁一二面都没有让写算法题，可能是笔试满分的原因。
5. 壹场科技，一家小厂主要是海外业务的，面试体验也非常好，面试过程中问完简历的内容就会开始问开放性的场景题目，是他们目前做的项目遇到的问题和扩展，面试官会一起思考和引导，面试过程很开心，面试官的引导会让信心大增。

# 2. 算法题

算法题主要是刷了一遍 LeetCode 的 Top100，以及牛客网的 Top101，两者的题库有一些是重叠的，也刷了一遍代码随想录和算法小抄的书上题目。机器学习的算法主要是练习了KNN、BN、LN、SGD、LR、SVM和朴素贝叶斯。列一下面试遇到的没准备的题目和出现的问题：

**Q：计算IOU**

```python
def cal_IOU(x1,y1,x2,y2,x3,y3,x4,y4):
	# x1,y1,x2,y2是一个矩形框的左上和右下点
	# x3,y3,x4,y4是另外一个矩形框的左上和右下点
	xi1 = max(x1,x3)
	yi1 = max(y1,y3)
	xi2 = min(x2,x4)
	yi2 = min(y2,y4)
	# xi1,yi1,xi2,yi2是inner的左上和右下点
	inner = (xi2-xi1)*(yi2-yi1) #面积
	outer = (x2-x1)*(y2-y1)+(x4-x3)*(y4-y3)-inner
	IOU = inner/outer
	return IOU
```

**Q：三角形内均匀随机取点**

看到题目，可能会没理解题意

```python
import random
import numpy as np
def rec_rand(p0,p1,p2):
	# 输入是构成三角形的三个点的坐标,是numpy数组
	x = random.uniform(0,1)
	y = random.uniform(0,1-x)
	z = 1-x-y
	return x*p0+y*p1+z*p2
```

**Q：124. 二叉树中的最大路径和**

这个题目应该都刷过，但是需要注意一点是这个题在牛客网的python的递归解法是会超递归栈的，要么换成c++代码，要么剪枝或者备忘录操作。

```cpp
class Solution {
public:
    int maxPathSum(TreeNode* root) {
        int sum = INT_MIN;
        dfs(root,sum);
        return sum;
    }
    int dfs(TreeNode* root,int& sum){
        if(root==NULL)return 0;
        int L = max(0,dfs(root->left,sum));
        int R = max(0,dfs(root->right,sum));
        sum = max(sum,L+R+root->val);
        return max(L,R)+root->val;
    }
};
```

其他的题目都是LeetCode的题目。

# 3. 数学题

其实就只碰到一道，不过和之前见到的不太相同。

**Q：甲、乙两个人每次都可以往初始为0的数字上选择1，2，3，4四个数字中的一个加上去，当添加上数字后总和大于等于100的人输掉，那么问是先手必赢还是后手必赢。**

A：答案应该是后手必赢，因为后手可以控制数字的总和为99，这样先手就必超过100。具体证明就不太清楚啦。

# 4. 八股题

大部分都是在简历上的内容进行扩展的，因为简历有强化学习内容以及LSTM的内容，所以这方面的扩展会多一些。

- 讲一下 LSTM

  - LSTM算法包含多个公式，以下是其中几个常见的LSTM公式：

    1. 输入门公式： it=σ(Wxixt+Whiht−1+Wcict−1+bi)i_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i)it=σ(Wxixt+Whiht−1+Wcict−1+bi)
    2. 遗忘门公式： ft=σ(Wxfxt+Whfht−1+Wcfct−1+bf)f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1} + b_f)ft=σ(Wxfxt+Whfht−1+Wcfct−1+bf)
    3. 更新单元状态公式： ct=ft⊙ct−1+it⊙tanh(Wxcxt+Whcht−1+bc)c_t = f_t \odot c_{t-1} + i_t \odot \mathrm{tanh}(W_{xc}x_t + W_{hc}h_{t-1} + b_c)ct=ft⊙ct−1+it⊙tanh(Wxcxt+Whcht−1+bc)
    4. 输出门公式： ot=σ(Wxoxt+Whoht−1+Wcoct+bo)o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_t + b_o)ot=σ(Wxoxt+Whoht−1+Wcoct+bo)
    5. 隐藏状态输出公式： ht=ot⊙tanh(ct)h_t = o_t \odot \mathrm{tanh}(c_t)ht=ot⊙tanh(ct)

    其中，

    σ\sigmaσ

    为sigmoid函数，

    ⊙\odot⊙

    表示逐元素乘积，

    WWW

    和 

    bbb

     是网络中的权重和偏置。

- LSTM与RNN的区别和优缺点

  - LSTM（Long Short-Term Memory）和RNN（Recurrent Neural Network）都是序列数据的神经网络模型，用于处理时间相关性。

    LSTM相对于RNN的主要区别在于LSTM包含了一个记忆单元和三个门控单元：输入门、遗忘门和输出门。这些门控单元可以选择性地控制信息流入和流出记忆单元，能够有效地解决长期依赖问题，避免梯度消失或爆炸的问题，并且能够捕捉到序列中的有用信息。

    相比之下，RNN模型则存在梯度消失和梯度爆炸等问题，由于信息只能通过隐藏状态传递，不能有效地处理长期依赖关系，从而影响其对序列数据进行建模的能力。

    LSTM的优点在于能够处理长期依赖关系，能够捕捉到序列中的有用信息，并且能够防止梯度消失或爆炸的问题。缺点在于参数较多，计算复杂度相对较高，训练和调整网络结构较为困难。

    RNN的优点在于模型简单，适合处理逐个时间步的数据，并且具有比较好的可解释性；缺点在于梯度消失或爆炸的问题、无法处理较长序列数据以及在数据量较大的时候容易过拟合。

    总的来说，LSTM相比于RNN具有更好的建模能力，但训练和调整网络结构相对较为困难，而RNN模型相对简单，适合处理逐个时间步的数据，但是在处理长期依赖关系和较长序列数据方面效果不如LSTM。

- Kmeans聚类算法一定收敛吗

  - K均值（K-means）是一种聚类分析方法，通过将n个数据对象分成k个簇使得所获得的簇满足：同一簇内的对象相似度较高且簇间的相似度较低。具体实现过程大致如下：首先随机选择k个点作为初始聚类中心，然后将样本点分配给离它最近的聚类中心，再根据聚类结果更新聚类中心，并重复以上步骤直至聚类中心不再发生变化或达到设定的迭代次数。

    通常情况下，K均值算法可以收敛到局部最优解，但并不保证一定会收敛到全局最优解。事实上，对于不同的初始聚类中心，可能会导致聚类结果不同，即最终的聚类中心及簇划分可能会发生变化。因此，在实际应用中，我们通常需要多次运行K均值算法，并比较得到的聚类结果，以选择最优的聚类方案。

    此外，如果初始聚类中心的选择不合理，可能会导致算法无法收敛或收敛很慢。因此，在实际应用中，我们需要注意选择合适的初始聚类中心，例如可以通过KMeans++等改进算法来选择初始聚类中心，提高算法的稳定性和效果。

- 讲一下DBN模型

  - DBN（Deep Belief Network）即深度置信网络，是一种基于无监督学习的神经网络模型。它由多个受限玻尔兹曼机（RBM）级联而成，每层的输出作为下一层的输入，最后一层的输出可以用于分类、回归等任务。

    深度置信网络的训练分为两个阶段，分别是无监督的预训练和有监督的微调。

    在无监督的预训练阶段，从第一层开始，每次训练一个受限玻尔兹曼机，并将其输出作为下一层的输入进行训练，最终形成一个深度置信网络。这个阶段的训练目标是使得每一层网络都能够捕获到输入数据中的某些特征或者潜在变量，并且将这些信息传递给下一层网络。

    在有监督的微调阶段，使用反向传播算法对整个网络进行训练，来进一步优化网络的性能。该阶段的目标是通过有标签的数据来微调网络权值，从而使得网络的输出更符合真实标签。通常情况下，微调阶段的训练需要迭代多次，通过不断地调整网络参数来提高网络的精度。

    深度置信网络具有很好的特征提取能力，可以在不需要人工特征工程的情况下，提取输入数据中的高层次特征，并且能够处理大量的高维数据。它被广泛应用于图像分类、语音识别、自然语言处理等领域。

- 知道整数规划吗

  （应该是简历里有线性规划的内容，所以想考一下扩展）

  - 整数规划是线性规划的一种扩展形式，在线性规划的基础上要求决策变量取整数值。整数规划常用于生产调度、货物运输、资源配置、网络流量等具有离散决策的优化问题。

    通常情况下，整数规划问题可以表示为如下标准形式的数学模型：

    min⁡cTxs.t.Ax=bxi∈Z, i=1,2,…,n.\begin{aligned} & \min \quad c^Tx \\ & \text{s.t.} \quad Ax=b \\ & \quad\quad \quad x_i \in \mathbb{Z}, \ i=1,2,\ldots,n. \end{aligned}mincTxs.t.Ax=bxi∈Z, i=1,2,…,n.

    其中ccc为nnn维列向量，xxx为nnn维整数向量，AAA为m×nm\times nm×n的矩阵，bbb为mmm维列向量。在这个模型中，我们的目标是使目标函数cTxc^TxcTx最小化，同时满足Ax=bAx=bAx=b和xi∈Zx_i\in\mathbb{Z}xi∈Z的约束条件。

    整数规划问题的求解比线性规划问题更加困难，因为整数规划问题是NP-hard的。常见的求解方法包括分支定界法、割平面法、随机搜索法、动态规划等。此外，也可以将整数规划问题转化为其他形式的数学模型进行求解，例如混合整数线性规划（MILP）、整数非线性规划（INLP）等。

    在实际应用中，我们需要根据具体问题的特点和求解目标选择合适的算法，并结合启发式算法、元启发式算法等技巧来提高求解效率。

- 介绍一下推荐系统的理解

  - 召回、粗排和精排是信息检索系统中常用的三个环节，通常被用于搜索引擎、推荐系统等场景中。

    召回（Recall）是信息检索系统中的第一步，其目标是从海量数据集中尽可能地找出与查询条件相关的数据，并将其返回给用户。召回算法通常会通过分析用户的查询条件和候选数据的特征来筛选出可能相关的部分数据子集。召回算法注重召回率，即查找到的相关数据数量比例。主要是使用双塔模型，目标函数是精排的排序序列

    粗排（Coarse Ranking）是在召回阶段的基础上对数据进行粗略排名的算法。通常采用传统的排序算法，例如基于TF-IDF权重、BM25模型等，将召回的数据按照相关性从高到低进行排序，以便后续精排阶段更加高效地进行处理。粗排算法注重速度和排序准确性。主要是LR等模型，目标函数也是精排的排序序列

    精排（Fine Ranking）是在粗排阶段和用户交互之间的一个重要环节，其目标是进一步提高排序的准确性，并根据用户的需求进行个性化的排序。精排算法通常会采用机器学习、深度学习等技术来调整排序结果。精排算法注重排序准确性和个性化匹配度。可以上大模型，FM、DIN之类的，目标是预测CTR等指标。

    综上所述，召回、粗排和精排是三个不同层次、不同目标的信息检索阶段，它们共同构成了一个完整的信息检索系统，可以提供高效准确的搜索结果。

- 介绍一下ChatGPT中的RLHF

  - ChatGPT是OpenAI团队开发的一个基于GPT模型在对话场景下微调得到的模型，RLHF是ChatGPT中应用的一种强化学习算法。

    相比传统的强化学习算法，RLHF将人类反馈与强化学习相结合，在对话场景中更快、更稳定地训练模型，提高模型的性能。具体来说，RLHF分为以下三个步骤：

    1. **探索**（Exploration）：模型根据当前状态和策略生成回复，并发送给用户，等待用户反馈。
    2. **反馈**（Feedback）：用户对模型生成的回复进行评价，并返回给模型。
    3. **更新**（Update）：利用用户的反馈来更新模型的权重和策略，从而提高模型的性能。

    通过这个过程，RLHF可以让模型更加快速、稳定地学习对话场景中的策略，从而提高自动对话系统的质量和效率。而PPO算法则是RLHF中采用的一种优化算法，可以有效地解决传统的policy gradient算法在训练过程中的不稳定问题。

- 介绍一下PPO算法

  - PPO（Proximal Policy Optimization，近端策略优化）是一种常用的策略优化算法，适用于强化学习和对话系统等多种场景。它是on-policy的算法

    相比传统的policy gradient算法，PPO的优势在于提高了训练的效率和稳定性，同时还能够有效避免出现不稳定、振荡等问题。

    PPO的核心思想是通过限制更新步长，从而保证每次更新后的策略与原来的策略不会有太大的改变。具体来说，它通过两个重要的技术来实现近端策略优化：

    1. **Clipping objective function（目标函数裁剪）**：在训练过程中，PPO会限制当前策略更新时的KL散度大小，从而避免策略改变过快、波动过大。这是通过对目标函数进行裁剪实现的，从而保证策略更新的稳定性和可靠性。
    2. **Value function clipping（值函数裁剪）**：PPO还利用一个价值函数来估计每个状态的价值，从而指导策略的更新。在训练过程中，PPO会将当前状态的优势估计与我们预期的优势值进行比较，并通过裁剪来避免更新后出现不良的效果。

    通过这些技术的应用，PPO能够在保证稳定性和效率的同时，提高模型的训练速度和策略的效用。在强化学习等场景下，PPO已经成为了一种广泛使用的策略优化算法。

- 除了往多模态开发以外，ChatGPT还可以有哪些发展方向或者改进思路（仅供参考）

  - 集成promote，使人类少进行promote，一种可行的方法是，根据人们的问题，内部加载对应的promote。
  - 增强安全性，OpenAI对ChatGPT进行了安全性限制，但是通过promote可以绕过这个限制，可以通过对抗训练的方式增强安全性。

上面就是一些有意思的扩展了，目前各个厂商都在做自己的大语言模型，所以在面试的内容中都会一些ChatGPT的内容，最好去了解一下大致的算法流程，基本上每次面试都会让说一遍它的算法流程。明明岗位是机器学习还是需要记一些NLP的内容。

顺带说一下，蚂蚁的一些面试官好像会看简历的自我评价这一栏，我写了喜欢读书，结果就问了最近读什么书，感想是什么，有没有什么收获。

作者：莱维_
链接：https://www.nowcoder.com/discuss/475974027329265664?sourceSSR=search
来源：牛客网